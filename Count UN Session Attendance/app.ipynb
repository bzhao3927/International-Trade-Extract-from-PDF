{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.1-cp39-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.1-cp39-abi3-macosx_10_9_x86_64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key=OPENAI_API_KEY)\n",
    "\n",
    "class DelegationSession(BaseModel):\n",
    "    country: str = Field(description=\"Country name.\")\n",
    "    officials: Optional[List[str]] = Field(default_factory=list)\n",
    "    representatives: Optional[List[str]] = Field(default_factory=list)\n",
    "    alternate_representatives: Optional[List[str]] = Field(default_factory=list)\n",
    "    advisers: Optional[List[str]] = Field(default_factory=list)\n",
    "    other_attendees: Optional[List[str]] = Field(default_factory=list)\n",
    "    leader_present: bool = Field(description=\"True if any official is president or prime minister\")\n",
    "    year: str = Field(default=\"NA\")\n",
    "\n",
    "class DelegationData(BaseModel):\n",
    "    sessions: List[DelegationSession]\n",
    "\n",
    "prompt_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=(\n",
    "        \"You are extracting UN delegation session data from the following text for the country: {country}.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Extract **all** names under each category: officials, representatives, alternate representatives, advisers, other attendees.\\n\"\n",
    "        \"2. The lists might be long and span multiple lines. Be exhaustive and include every name.\\n\"\n",
    "        \"3. If the text mentions 'President' or 'Prime Minister', set leader_present = true.\\n\"\n",
    "        \"4. Use 'NA' if year is missing.\\n\"\n",
    "        \"5. Return the result as JSON matching the schema.\\n\\n\"\n",
    "        \"Text:\\n{text}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([prompt_template])\n",
    "functions = [convert_to_openai_function(DelegationData)]\n",
    "gpt_func_model = model.bind(functions=functions, function_call={\"name\": \"DelegationData\"})\n",
    "extraction_chain = prompt | gpt_func_model | JsonKeyOutputFunctionsParser(key_name=\"sessions\")\n",
    "\n",
    "def extract_full_ocr_text(pdf_path, dpi=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Extract year from first page only (page 0)\n",
    "    first_page = doc.load_page(0)\n",
    "    pix = first_page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    first_page_text = pytesseract.image_to_string(img)\n",
    "\n",
    "    year_match = re.search(r\"\\b(19|20)\\d{2}(?:\\s*[-\\u2013]\\s*(19|20)\\d{2})?\\b\", first_page_text)\n",
    "    if year_match:\n",
    "        year = year_match.group(0)\n",
    "        if \"-\" in year or \"–\" in year:\n",
    "            year = re.split(r\"[-–]\", year)[0].strip()\n",
    "    else:\n",
    "        year = \"NA\"\n",
    "\n",
    "    # Extract text starting from page 5 (index 4)\n",
    "    full_text = \"\"\n",
    "    for i in range(4, len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        page_text = pytesseract.image_to_string(img)\n",
    "        full_text += \"\\n\" + page_text\n",
    "\n",
    "    return full_text, year\n",
    "\n",
    "def clean_text_no_internal_blank_lines(raw_text):\n",
    "    lines = [line.strip() for line in raw_text.splitlines()]\n",
    "    lines = [line for line in lines if line and not re.fullmatch(r'\\d+', line)]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def split_text_by_Country(text):\n",
    "    cleaned_text = clean_text_no_internal_blank_lines(text)\n",
    "    country_pattern = r'^(?P<country>[A-Z][A-Z\\s\\-&.,]*)$'\n",
    "\n",
    "    decorated_text = re.sub(\n",
    "        country_pattern,\n",
    "        lambda m: f\"\\n~~~~~ {m.group('country').strip()}\",\n",
    "        cleaned_text,\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "\n",
    "    chunks = decorated_text.split(\"~~~~~\")\n",
    "    final_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    spaced_text = \"\\n\\n\".join(f\"~~~~~ {chunk}\" for chunk in final_chunks)\n",
    "    return final_chunks, spaced_text\n",
    "\n",
    "def filter_chunks_by_last_country(chunks, last_country=\"ZIMBABWE\"):\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.splitlines()\n",
    "        if not lines:\n",
    "            continue\n",
    "        country = lines[0].strip().upper()\n",
    "        filtered_chunks.append(chunk)\n",
    "        if country == last_country.upper():\n",
    "            break  # stop after last country\n",
    "    return filtered_chunks\n",
    "\n",
    "def extract_sessions_from_text_chunks(chunks):\n",
    "    all_sessions = []\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.splitlines()\n",
    "        country = lines[0].strip() if lines else \"NA\"\n",
    "        chunk_text = '\\n'.join(lines[1:]).strip() if len(lines) > 1 else \"\"\n",
    "        try:\n",
    "            sessions = extraction_chain.invoke({\n",
    "                \"country\": country,\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "            all_sessions.extend(sessions)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] LangChain extraction failed for country {country}: {e}\")\n",
    "    return all_sessions\n",
    "\n",
    "def merge_sessions(sessions: List[dict]) -> List[dict]:\n",
    "    merged = {}\n",
    "    for s in sessions:\n",
    "        country = s.get(\"country\") or \"NA\"\n",
    "        year = s.get(\"year\") or \"NA\"\n",
    "        key = (country, year)\n",
    "        if key not in merged:\n",
    "            merged[key] = {\n",
    "                \"country\": country,\n",
    "                \"year\": year,\n",
    "                \"officials\": set(s.get(\"officials\") or []),\n",
    "                \"representatives\": set(s.get(\"representatives\") or []),\n",
    "                \"alternate_representatives\": set(s.get(\"alternate_representatives\") or []),\n",
    "                \"advisers\": set(s.get(\"advisers\") or []),\n",
    "                \"other_attendees\": set(s.get(\"other_attendees\") or []),\n",
    "                \"leader_present\": s.get(\"leader_present\", False)\n",
    "            }\n",
    "        else:\n",
    "            for k in [\"officials\", \"representatives\", \"alternate_representatives\", \"advisers\", \"other_attendees\"]:\n",
    "                merged[key][k].update(s.get(k) or [])\n",
    "            merged[key][\"leader_present\"] |= s.get(\"leader_present\", False)\n",
    "    return [{\n",
    "        \"country\": k[0],\n",
    "        \"year\": k[1],\n",
    "        \"officials\": sorted(v[\"officials\"]),\n",
    "        \"representatives\": sorted(v[\"representatives\"]),\n",
    "        \"alternate_representatives\": sorted(v[\"alternate_representatives\"]),\n",
    "        \"advisers\": sorted(v[\"advisers\"]),\n",
    "        \"other_attendees\": sorted(v[\"other_attendees\"]),\n",
    "        \"leader_present\": v[\"leader_present\"]\n",
    "    } for k, v in merged.items()]\n",
    "\n",
    "def save_sessions_to_excel(sessions, filename=None):\n",
    "    if not filename:\n",
    "        filename = f\"UN_Session_Counts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "    rows = []\n",
    "    for session_id, s in enumerate(sessions, start=1):\n",
    "        attendees_count = sum(len(s.get(k) or []) for k in [\"officials\", \"representatives\", \"alternate_representatives\", \"advisers\"])\n",
    "        rows.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"country\": s[\"country\"].title(),  # ✅ Title-case the country name\n",
    "            \"year\": s[\"year\"],\n",
    "            \"officials\": len(s[\"officials\"]),\n",
    "            \"leader_present\": int(s[\"leader_present\"]),\n",
    "            \"representatives\": len(s[\"representatives\"]),\n",
    "            \"alternate_representatives\": len(s[\"alternate_representatives\"]),\n",
    "            \"advisers\": len(s[\"advisers\"]),\n",
    "            \"attendees\": attendees_count\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_excel(filename, index=False)\n",
    "    return filename, df\n",
    "\n",
    "def process_uploaded_pdfs(pdf_files):\n",
    "    if not pdf_files:\n",
    "        return [], None, None\n",
    "\n",
    "    all_sessions = []\n",
    "    cleaned_prompt_texts = []\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        full_text, detected_year = extract_full_ocr_text(pdf_path)\n",
    "\n",
    "        chunks, cleaned_marked_text = split_text_by_Country(full_text)\n",
    "        chunks = filter_chunks_by_last_country(chunks, last_country=\"ZIMBABWE\")\n",
    "\n",
    "        # Regenerate cleaned_marked_text after filtering\n",
    "        cleaned_marked_text = \"\\n\\n\".join(f\"~~~~~ {chunk}\" for chunk in chunks)\n",
    "        cleaned_prompt_texts.append(cleaned_marked_text)\n",
    "\n",
    "        sessions = extract_sessions_from_text_chunks(chunks)\n",
    "        for s in sessions:\n",
    "            if s[\"year\"] == \"NA\":\n",
    "                s[\"year\"] = detected_year\n",
    "\n",
    "        all_sessions.extend(sessions)\n",
    "\n",
    "    merged_sessions = merge_sessions(all_sessions)\n",
    "    excel_path, df = save_sessions_to_excel(merged_sessions)\n",
    "\n",
    "    cleaned_text_path = f\"cleaned_prompt_text_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    with open(cleaned_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in cleaned_prompt_texts:\n",
    "            f.write(text + \"\\n\\n=====\\n\\n\")\n",
    "\n",
    "    return df.to_dict(orient=\"records\"), excel_path, cleaned_text_path\n",
    "\n",
    "def create_gradio_app():\n",
    "    with gr.Blocks(title=\"UN Delegation Extractor\") as demo:\n",
    "        gr.Markdown(\"## UN Delegation Session Extractor (LangChain + GPT-4o + OCR)\")\n",
    "        pdf_input = gr.File(label=\"Upload scanned UN Session PDFs\", file_types=[\".pdf\"], file_count=\"multiple\", type=\"filepath\")\n",
    "        extract_btn = gr.Button(\"Extract Delegation Data\")\n",
    "\n",
    "        result_json = gr.JSON(label=\"Extracted Structured Data\")\n",
    "        download_excel = gr.File(label=\"Download Excel Summary\", visible=True)\n",
    "        download_cleaned = gr.File(label=\"Download Cleaned Text for GPT Input\", visible=True)\n",
    "\n",
    "        extract_btn.click(\n",
    "            fn=process_uploaded_pdfs,\n",
    "            inputs=[pdf_input],\n",
    "            outputs=[result_json, download_excel, download_cleaned]\n",
    "        )\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = create_gradio_app()\n",
    "    app.launch(share=True, inbrowser=True, show_error=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
